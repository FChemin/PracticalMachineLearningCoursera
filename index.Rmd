---
title: "Prediction Assignment Writeup - How weight lifting was performed"
author: "FChemin"
date: "27/12/2016"
output: html_document
keep md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to predict the manner in which they did the exercise: participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions (exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E)).

## Data cleaning

The data for this project come from [link](http://groupware.les.inf.puc-rio.br/har):  

```{r}
# Loading training and testing data sets

setwd("~/Documents/Coursera/datasciencecoursera/PracticalMachineLearning")
trainURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(trainURL, "pml-training.csv", method = "curl")
download.file(testURL, "pml-testing.csv", method = "curl")

training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
```

Now we want to clean the data and keep only relevant variables, that is, the one without NAs and participants informations:

```{r}
NAs <- apply(training,2,function(x) {sum(is.na(x))})
noNAtrain <- training[,which(NAs == 0)]
cleanTrain <- noNAtrain[,-c(1:7)]

indx <- sapply(testing, is.logical)
testing[indx] <- lapply(testing[indx], function(x) as.factor(x))
noNATest <- testing[,which(NAs == 0)]
cleanTest <- noNATest[,-c(1:7)]
```

## Creating our model(s)

Now that we have a clean training set, we want to divide it to create a validation set:

```{r}
library(caret)
library(doParallel)

inTrain <- createDataPartition(cleanTrain$classe, p = 0.7, list = FALSE)

trainset <- cleanTrain[inTrain,]
dim(trainset)

validationset <- cleanTrain[-inTrain,]
dim(validationset)
```

We will train a Random Forest algorithm using cross-validation on our training set and use it on our validation set to predict the outcomes. Th

```{r}
library(snow)
cl1 <- makeCluster(8)
set.seed(1234)
ctrl <- trainControl(method = "repeatedcv", number = 3, allowParallel = TRUE, verboseIter = TRUE)
modFit <- train(classe~., data = trainset, method="rf", trControl = ctrl)
modFit
stopCluster(cl1)
```

The report shows us that **the model accuracy is of 98.51%**. We are ok with that and won't make another model.
Now we want to test the accuracy of our model on the validation set before using it to make our predictions on the test set:

```{r}
pred1 <- predict(modFit, validationset[,1:85])
confusionMatrix(pred1, validationset$classe)
```

From the confusion matrix, we can see that the accuracy is equal to 98.73% and comprise in a 95% interval between 98.41% and 98.99%. **We would expect the out-of-sample accuracy of the model to be below 98.41%, to account for overfitting in the model**.

## Prediction Time!

Now we use our model on the 20 examples from the Quizz:

```{r results="hide"}
for(i in 1:85){
        levels(cleanTest[,i]) <- levels(trainset[,i])
}
cleanTest <- na.roughfix(cleanTest)
pred2 <- predict(modFit, cleanTest[1:85])
```